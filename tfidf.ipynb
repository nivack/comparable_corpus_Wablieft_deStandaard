{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataframes\\\\final_wa_df.pickle', 'rb') as handle:\n",
    "    all_wa_df = pickle.load(handle)\n",
    "\n",
    "with open('dataframes\\\\final_stan_df.pickle', 'rb') as handle:\n",
    "    all_stan_df = pickle.load(handle)\n",
    "\n",
    "\n",
    "wa_content = all_wa_df.content\n",
    "stan_content = all_stan_df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>wa936bik6.txt</td>\n",
       "      <td>Minder kinderen geboren</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>België</td>\n",
       "      <td>Minder kinderen geboren In Vlaamse ziekenhuize...</td>\n",
       "      <td>[0.8717681169509888, 0.21637214720249176, 0.67...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10113</th>\n",
       "      <td>wa936bik7.txt</td>\n",
       "      <td>Vlaams medicijn tegen tbc</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>België</td>\n",
       "      <td>Vlaams medicijn tegen tbc Onderzoekers van het...</td>\n",
       "      <td>[-0.1551680862903595, 0.6744857430458069, 0.13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10114</th>\n",
       "      <td>wa936bik8.txt</td>\n",
       "      <td>Directrice verdacht van spieken</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>België</td>\n",
       "      <td>Directrice verdacht van spieken De directrice ...</td>\n",
       "      <td>[-0.3453265130519867, -0.008550132624804974, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10118</th>\n",
       "      <td>wa936bin3.txt</td>\n",
       "      <td>Weinig vrouwen met sjerp in ons land</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>België</td>\n",
       "      <td>Weinig vrouwen met sjerp in ons land In januar...</td>\n",
       "      <td>[-0.2693834602832794, 0.9814454913139343, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10116</th>\n",
       "      <td>wa936bin1.txt</td>\n",
       "      <td>Noémie is Miss België</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>België</td>\n",
       "      <td>Noémie is Miss België Noémie Happart is de moo...</td>\n",
       "      <td>[-0.3678942322731018, -0.24943530559539795, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6084</th>\n",
       "      <td>wa1164bui5.txt</td>\n",
       "      <td>Duizenden kleine pinguïns dood</td>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>Buitenland</td>\n",
       "      <td>Duizenden kleine pinguïns dood Een grote ramp ...</td>\n",
       "      <td>[-0.21590065956115723, 0.23220542073249817, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>wa1164buk1.txt</td>\n",
       "      <td>Nog meer uitstel in Congo</td>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>Buitenland</td>\n",
       "      <td>Nog meer uitstel in Congo In Congo komen er ge...</td>\n",
       "      <td>[-1.5455697774887085, -0.9892030358314514, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>wa1164buk2.txt</td>\n",
       "      <td>Nieuwe regering in Nederland</td>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>Buitenland</td>\n",
       "      <td>Nieuwe regering in Nederland Nederland heeft e...</td>\n",
       "      <td>[-1.0460810661315918, -0.7230560779571533, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>wa1164idk1.txt</td>\n",
       "      <td>Ik wil onrecht aanklagen, maar liefst met een ...</td>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>In de kijker</td>\n",
       "      <td>\"Ik wil onrecht aanklagen, maar liefst met een...</td>\n",
       "      <td>[-0.40642863512039185, -0.12022947520017624, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>wa1164we7.txt</td>\n",
       "      <td>Een wolk tegen muggen</td>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>Weetjes</td>\n",
       "      <td>Een wolk tegen muggen Een jongen loopt in Indi...</td>\n",
       "      <td>[0.1634736955165863, 0.29441991448402405, 0.15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8744 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename                                              title  \\\n",
       "10112   wa936bik6.txt                            Minder kinderen geboren   \n",
       "10113   wa936bik7.txt                          Vlaams medicijn tegen tbc   \n",
       "10114   wa936bik8.txt                    Directrice verdacht van spieken   \n",
       "10118   wa936bin3.txt               Weinig vrouwen met sjerp in ons land   \n",
       "10116   wa936bin1.txt                              Noémie is Miss België   \n",
       "...               ...                                                ...   \n",
       "6084   wa1164bui5.txt                     Duizenden kleine pinguïns dood   \n",
       "6085   wa1164buk1.txt                          Nog meer uitstel in Congo   \n",
       "6086   wa1164buk2.txt                       Nieuwe regering in Nederland   \n",
       "6088   wa1164idk1.txt  Ik wil onrecht aanklagen, maar liefst met een ...   \n",
       "6106    wa1164we7.txt                              Een wolk tegen muggen   \n",
       "\n",
       "            date      category  \\\n",
       "10112 2013-01-08        België   \n",
       "10113 2013-01-08        België   \n",
       "10114 2013-01-08        België   \n",
       "10118 2013-01-08        België   \n",
       "10116 2013-01-08        België   \n",
       "...          ...           ...   \n",
       "6084  2017-11-16    Buitenland   \n",
       "6085  2017-11-16    Buitenland   \n",
       "6086  2017-11-16    Buitenland   \n",
       "6088  2017-11-16  In de kijker   \n",
       "6106  2017-11-16       Weetjes   \n",
       "\n",
       "                                                 content  \\\n",
       "10112  Minder kinderen geboren In Vlaamse ziekenhuize...   \n",
       "10113  Vlaams medicijn tegen tbc Onderzoekers van het...   \n",
       "10114  Directrice verdacht van spieken De directrice ...   \n",
       "10118  Weinig vrouwen met sjerp in ons land In januar...   \n",
       "10116  Noémie is Miss België Noémie Happart is de moo...   \n",
       "...                                                  ...   \n",
       "6084   Duizenden kleine pinguïns dood Een grote ramp ...   \n",
       "6085   Nog meer uitstel in Congo In Congo komen er ge...   \n",
       "6086   Nieuwe regering in Nederland Nederland heeft e...   \n",
       "6088   \"Ik wil onrecht aanklagen, maar liefst met een...   \n",
       "6106   Een wolk tegen muggen Een jongen loopt in Indi...   \n",
       "\n",
       "                                              embeddings  \n",
       "10112  [0.8717681169509888, 0.21637214720249176, 0.67...  \n",
       "10113  [-0.1551680862903595, 0.6744857430458069, 0.13...  \n",
       "10114  [-0.3453265130519867, -0.008550132624804974, -...  \n",
       "10118  [-0.2693834602832794, 0.9814454913139343, -0.1...  \n",
       "10116  [-0.3678942322731018, -0.24943530559539795, -0...  \n",
       "...                                                  ...  \n",
       "6084   [-0.21590065956115723, 0.23220542073249817, -0...  \n",
       "6085   [-1.5455697774887085, -0.9892030358314514, 0.0...  \n",
       "6086   [-1.0460810661315918, -0.7230560779571533, 0.5...  \n",
       "6088   [-0.40642863512039185, -0.12022947520017624, 0...  \n",
       "6106   [0.1634736955165863, 0.29441991448402405, 0.15...  \n",
       "\n",
       "[8744 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_wa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NaNs\n",
    "\n",
    "all_wa_df = all_wa_df.dropna(subset=['title'])\n",
    "all_stan_df = all_stan_df.dropna(subset=['title'])\n",
    "\n",
    "all_wa_df = all_wa_df[all_wa_df.title != 'No title']\n",
    "all_stan_df = all_stan_df[all_stan_df.title != 'No title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing functions\n",
    "\n",
    "def remove_accents(text):\n",
    "    text= unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "\n",
    "    text = re.sub(pattern, '',text)\n",
    "\n",
    "    return text\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('dutch')\n",
    "\n",
    "def remove_stopwords(text, lower_case=False):\n",
    "    tokens = nltk.word_tokenize(text, language='dutch')\n",
    "    tokens = [token.strip().strip(\"'\").lower() for token in tokens]\n",
    "\n",
    "    if lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "\n",
    "    #turn it back into 1 string to give as input to the sklearn vectorizer\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wa_df['title_no_stop'] = all_wa_df['title'].apply(lambda x: remove_accents(x))\n",
    "all_wa_df['title_no_stop'] = all_wa_df['title_no_stop'].apply(lambda x: remove_special_characters(x))\n",
    "all_wa_df['title_no_stop'] = all_wa_df['title_no_stop'].apply(lambda x: remove_stopwords(x, True))\n",
    "\n",
    "all_stan_df['title_no_stop'] = all_stan_df['title'].apply(lambda x: remove_accents(x))\n",
    "all_stan_df['title_no_stop'] = all_stan_df['title_no_stop'].apply(lambda x: remove_special_characters(x))\n",
    "all_stan_df['title_no_stop'] = all_stan_df['title_no_stop'].apply(lambda x: remove_stopwords(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wa_df['content_no_stop'] = all_wa_df['content'].apply(lambda x: remove_accents(x))\n",
    "all_wa_df['content_no_stop'] = all_wa_df['content_no_stop'].apply(lambda x: remove_special_characters(x))\n",
    "all_wa_df['content_no_stop'] = all_wa_df['content_no_stop'].apply(lambda x: remove_stopwords(x, True))\n",
    "\n",
    "all_stan_df['content_no_stop'] = all_stan_df['content'].apply(lambda x: remove_accents(x))\n",
    "all_stan_df['content_no_stop'] = all_stan_df['content_no_stop'].apply(lambda x: remove_special_characters(x))\n",
    "all_stan_df['content_no_stop'] = all_stan_df['content_no_stop'].apply(lambda x: remove_stopwords(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wa_df[\"Newspaper\"] = \"Wablieft\"\n",
    "all_stan_df[\"Newspaper\"] = \"Standaard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([all_wa_df, all_stan_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('allPreprocessedDf.pickle', 'wb') as handle:\n",
    "#     pickle.dump(all_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('dataframes\\\\allPreprocessedDf.pickle', 'rb') as handle:\n",
    "    all_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content = all_df[\"content_no_stop\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-IDF Titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_titles = all_wa_df[\"title_no_stop\"].tolist()\n",
    "\n",
    "vector = TfidfVectorizer(max_df=0.3,\n",
    "                             stop_words=stopword_list, \n",
    "                             lowercase=True,\n",
    "                             use_idf=True,\n",
    "                             norm=u'l2',\n",
    "                             smooth_idf=True\n",
    "                            )\n",
    "tfidf = vector.fit_transform(wa_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models\\\\tfidfVectorizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(vector, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_titles = all_stan_df[\"title_no_stop\"].tolist()\n",
    "\n",
    "stan_transformed = vector.transform(stan_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataframes\\manualRatedDf.pickle', 'rb') as handle:\n",
    "    manualRatedDf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualRatedDf = manualRatedDf.dropna(subset=['WaTitle'])\n",
    "manualRatedDf = manualRatedDf.dropna(subset=['StanTitle'])\n",
    "\n",
    "manualRatedDf = manualRatedDf[manualRatedDf.WaTitle != 'No title']\n",
    "manualRatedDf = manualRatedDf[manualRatedDf.StanTitle != 'No title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediateDf = pd.DataFrame()\n",
    "\n",
    "intermediateDf['WaTitleNoStop'] = manualRatedDf['WaTitle'].apply(lambda x: remove_accents(x))\n",
    "intermediateDf['WaTitleNoStop'] = intermediateDf['WaTitleNoStop'].apply(lambda x: remove_special_characters(x))\n",
    "intermediateDf['WaTitleNoStop'] = intermediateDf['WaTitleNoStop'].apply(lambda x: remove_stopwords(x, True))\n",
    "\n",
    "intermediateDf['StanTitleNoStop'] = manualRatedDf['StanTitle'].apply(lambda x: remove_accents(x))\n",
    "intermediateDf['StanTitleNoStop'] = intermediateDf['StanTitleNoStop'].apply(lambda x: remove_special_characters(x))\n",
    "intermediateDf['StanTitleNoStop'] = intermediateDf['StanTitleNoStop'].apply(lambda x: remove_stopwords(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11412\\3666434580.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaTitleList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mwa_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwaTitleList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mstan_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstanTitleList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtfidfSimList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwa_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstan_tfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vector' is not defined"
     ]
    }
   ],
   "source": [
    "tfidfSimList = []\n",
    "waTitleList = intermediateDf[\"WaTitleNoStop\"].tolist()\n",
    "stanTitleList = intermediateDf[\"StanTitleNoStop\"].tolist()\n",
    "\n",
    "for i in range(0, len(waTitleList)):\n",
    "    wa_tfidf = vector.transform([waTitleList[i]])\n",
    "    stan_tfidf = vector.transform([stanTitleList[i]])\n",
    "    tfidfSimList.append(linear_kernel(wa_tfidf, stan_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "simList = [simScore[0][0] for simScore in tfidfSimList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualRatedDf[\"tfidfTitle\"] = simList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataframes\\classifierDf.pickle', 'wb') as handle:\n",
    "    pickle.dump(manualRatedDf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add titletfidf to first evaluated articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('firstEvalDf.pickle', 'rb') as handle:\n",
    "    firstEvalDf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "waFileList = firstEvalDf.WaFile.tolist()\n",
    "stanFileList = firstEvalDf.StanFile.tolist()\n",
    "stanFileList= [ele.tolist()[0] for ele in stanFileList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=all_wa_df.loc[all_wa_df[\"filename\"] == waFileList[0]].title_no_stop.tolist()[0]\n",
    "test2=all_stan_df.loc[all_stan_df[\"filename\"] == stanFileList[0]].title_no_stop.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "print(linear_kernel(vector.transform([test1]), vector.transform([test2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfSimList = []\n",
    "for i in range(0, len(waFileList)):\n",
    "    wa_title = all_wa_df.loc[all_wa_df[\"filename\"] == waFileList[i]].title_no_stop.tolist()[0]\n",
    "    stan_title = all_stan_df.loc[all_stan_df[\"filename\"] == stanFileList[i]].title_no_stop.tolist()[0]\n",
    "    tfidfSimList.append(linear_kernel(vector.transform([wa_title]), vector.transform([stan_title])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "simList = [simScore[0][0] for simScore in tfidfSimList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstEvalDf[\"tfidfTitle\"] = simList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('firstEvalDf.pickle', 'wb') as handle:\n",
    "    pickle.dump(firstEvalDf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                          for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.90, min_df=20, max_features=5000)\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidfFitAllContent.pickle', 'wb') as handle:\n",
    "    pickle.dump(vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>090035987</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>100000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>zware</th>\n",
       "      <th>zwart</th>\n",
       "      <th>zwarte</th>\n",
       "      <th>zweden</th>\n",
       "      <th>zweedse</th>\n",
       "      <th>zwembad</th>\n",
       "      <th>zwemmen</th>\n",
       "      <th>zwijgen</th>\n",
       "      <th>zwitserland</th>\n",
       "      <th>zwitserse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.147796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184483</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184484</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184485</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184486</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184487</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184488 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         01   02  090035987   10  100      1000  10000  100000   11   12  ...  \\\n",
       "0       0.0  0.0        0.0  0.0  0.0  0.147796    0.0     0.0  0.0  0.0  ...   \n",
       "1       0.0  0.0        0.0  0.0  0.0  0.000000    0.0     0.0  0.0  0.0  ...   \n",
       "2       0.0  0.0        0.0  0.0  0.0  0.000000    0.0     0.0  0.0  0.0  ...   \n",
       "3       0.0  0.0        0.0  0.0  0.0  0.000000    0.0     0.0  0.0  0.0  ...   \n",
       "4       0.0  0.0        0.0  0.0  0.0  0.000000    0.0     0.0  0.0  0.0  ...   \n",
       "...     ...  ...        ...  ...  ...       ...    ...     ...  ...  ...  ...   \n",
       "184483  0.0  0.0        0.0  0.0  0.0  0.000000    0.0     0.0  0.0  0.0  ...   \n",
       "184484  0.0  0.0        0.0  0.0  0.0  0.000000    0.0     0.0  0.0  0.0  ...   \n",
       "184485  0.0  0.0        0.0  0.0  0.0  0.000000    0.0     0.0  0.0  0.0  ...   \n",
       "184486  0.0  0.0        0.0  0.0  0.0  0.000000    0.0     0.0  0.0  0.0  ...   \n",
       "184487  0.0  0.0        0.0  0.0  0.0  0.000000    0.0     0.0  0.0  0.0  ...   \n",
       "\n",
       "        zware  zwart  zwarte  zweden  zweedse  zwembad  zwemmen  zwijgen  \\\n",
       "0         0.0    0.0     0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "1         0.0    0.0     0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "2         0.0    0.0     0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "3         0.0    0.0     0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "4         0.0    0.0     0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "...       ...    ...     ...     ...      ...      ...      ...      ...   \n",
       "184483    0.0    0.0     0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "184484    0.0    0.0     0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "184485    0.0    0.0     0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "184486    0.0    0.0     0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "184487    0.0    0.0     0.0     0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "        zwitserland  zwitserse  \n",
       "0               0.0        0.0  \n",
       "1               0.0        0.0  \n",
       "2               0.0        0.0  \n",
       "3               0.0        0.0  \n",
       "4               0.0        0.0  \n",
       "...             ...        ...  \n",
       "184483          0.0        0.0  \n",
       "184484          0.0        0.0  \n",
       "184485          0.0        0.0  \n",
       "184486          0.0        0.0  \n",
       "184487          0.0        0.0  \n",
       "\n",
       "[184488 rows x 5000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "doc_term_matrix_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=list(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', max_iter=500, random_state=0).fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LDAmodel.pickle', 'wb') as handle:\n",
    "    pickle.dump(lda_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('LDAmodel.pickle', 'rb') as handle:\n",
    "    lda_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_matrix = lda_model.transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('allTfidfContentMatrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(tfidf_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('allLDAMatrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(lda_model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlist = lda_matrix.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"LDA\"] = lda_matrix.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "tolist not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1652\\2599441781.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Nick\\anaconda3\\envs\\thesis\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    685\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: tolist not found"
     ]
    }
   ],
   "source": [
    "tfidf_list = tfidf_matrix.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[\"titleTFIDF\"] = tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "we jaar zegt euro wel nieuwe procent mensen moeten onze twee gaat brussel miljoen waar\n",
      "Topic 1:\n",
      "veroordeeld cel rechtbank gevangenis rechter boete bedraagt maanden celstraf straf beringen focus kim beroep euro\n",
      "Topic 2:\n",
      "onderwijs leerlingen school studenten scholen leuven google leuvense ouders kinderen leerkrachten baby schauvliege beiden joke\n",
      "Topic 3:\n",
      "brand uur verkeer brandweer ongeval wagen mobiliteit politie auto nmbs rijden brug fietsers werkzaamheden bestuurder\n",
      "Topic 4:\n",
      "graden griekse oekraine wind nv delhaize heverlee opklaringen 3001 meteoservices 090035987 weerlijn wwwmeteoservicesbe kapeldreef maxima\n",
      "Topic 5:\n",
      "club anderlecht seizoen wedstrijd ploeg voetbal finale wk spelers ronde league spelen won rode standard\n",
      "Topic 6:\n",
      "politie man parket zone opgepakt verdachte agenten onderzoek slachtoffer verdacht ingediend verdachten dader vrouw onderzoeksrechter\n",
      "Topic 7:\n",
      "partij trump regering verkiezingen nva president premier parlement politieke europese partijen ps stemmen politiek eu\n",
      "Topic 8:\n",
      "russische rusland israel poetin gemeentebestuur kerk paus israelische militaire revolutie bbc moskou euthanasie moslims religieuze\n",
      "Topic 9:\n",
      "syrie vluchtelingen turkije aanslagen leger president syrische turkse irak vlucht aanslag asielzoekers conflict doden erdogan\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda_model, feature_names, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.90, min_df=100, max_features=5000, use_idf=True)\n",
    "\n",
    "tfidf_stan_matrix = vectorizer.fit_transform(all_stan_df.content_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', max_iter=500, random_state=0).fit(tfidf_stan_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stan_lda_model.pickle', 'wb') as handle:\n",
    "    pickle.dump(lda_model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3456e290f23e66e65c86db29512030f5b9518fa8a585a84d9a5b83ccae9d6ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
