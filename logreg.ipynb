{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics \n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataframes\\\\final_wa_df.pickle', 'rb') as handle:\n",
    "    wa_df= pickle.load(handle)\n",
    "\n",
    "with open('dataframes\\\\final_stan_df.pickle', 'rb') as handle:\n",
    "    stan_df = pickle.load(handle)\n",
    "\n",
    "wa_content = wa_df.content\n",
    "stan_content = stan_df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataframes\\classifierDf.pickle', 'rb') as handle:\n",
    "    manualRatedDf = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualRatedDf.loc[manualRatedDf[\"Match\"] == 0][\"DateDiff\"].describe(percentiles=[.75, .90, .95, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualRatedDf[[\"Score\", \"tfidfTitle\", \"DateDiff\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classifier test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LDA cosine sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = manualRatedDf[['Score', 'tfidfTitle', 'LDAMatch', 'DateDiff']]\n",
    "y = manualRatedDf[\"Match\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test=sklearn.model_selection.train_test_split(X,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = manualRatedDf[['Score', 'tfidfTitle']]\n",
    "y = manualRatedDf[\"Match\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test=sklearn.model_selection.train_test_split(X,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models\\logistic_regression', 'rb') as handle:\n",
    "    clf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{\n",
    "    # \"C\": [0.001,.01, .1, .5, 1, 10], \n",
    "    \"penalty\":[\"l2\", \"l1\"],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'cv':[5, 10, 15]\n",
    "}\n",
    "]\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    estimator=LogisticRegressionCV(),\n",
    "    param_grid=params,\n",
    "    refit=True,\n",
    "    n_jobs=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegressionCV(penalty='l2', solver=\"liblinear\", cv=10)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "#\n",
    "y_pred=logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"F1 score:\",metrics.f1_score(y_test, y_pred))\n",
    "print(\"Specificity:\",(cnf_matrix[0][0]/(cnf_matrix[0][0]+cnf_matrix[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.loc[X_test[\"Match\"] == 0][\"Score\"].describe(percentiles=[0.05,.10, 0.20, 0.5,.75,.90,.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[\"Match\"] = y_test.tolist()\n",
    "X_test.loc[X_test[\"Match\"] == 1][\"Score\"].describe(percentiles=[0.05,.10, 0.20, 0.5,.75,.90,.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=['No Match', \"Match\"] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"Logreg, auc=\"+str(auc))\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = logreg.intercept_[0]\n",
    "w = w1, w2, w3, w4 = logreg.coef_[0]\n",
    " \n",
    "equation = \"y = %f + (%f * x1) + (%f * x2) + (%f * x3) + (%f * x4)\" % (w0, w1, w2, w3, w4)\n",
    "print(equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=['Score', 'tfidfTitle', 'LDAMatch', 'DateDiff']\n",
    "feature_importance = pd.DataFrame(feature_names, columns = [\"Feature\"])\n",
    "feature_importance[\"Importance\"] = pow(math.e, w)\n",
    "feature_importance = feature_importance.sort_values(by = [\"Importance\"], ascending=False)\n",
    "\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = feature_importance.plot.barh(x='Feature', y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(logreg, open('models\\logistic_regression', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simscore thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = manualRatedDf[['Score', 'Match']]\n",
    "y = manualRatedDf[\"Match\"]\n",
    "\n",
    "X_train,X_test=sklearn.model_selection.train_test_split(X,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[X_train['Match'] == 0].describe(percentiles=[0.05, 0.10, .25, .50, .75, .90, .95, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[X_train['Match'] == 1].describe(percentiles=[0.05, 0.10, .25, .50, .75, .90, .95, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.value_counts(['Match'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.643\n",
    "y_test = X_test['Match']\n",
    "y_pred = [1 if float(score) >= threshold else 0 for score in X_test['Score'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"F1 score:\",metrics.f1_score(y_test, y_pred))\n",
    "print(\"Specificity:\",(cnf_matrix[0][0]/(cnf_matrix[0][0]+cnf_matrix[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUT_OFF VALUE 0.611:\n",
    "Accuracy: 0.8842105263157894\n",
    "Precision: 0.8823529411764706\n",
    "Recall: 0.9523809523809523\n",
    "F1 score: 0.916030534351145\n",
    "Specificity: 0.75\n",
    "\n",
    "CUT OFF VALUE 0.643\n",
    "Accuracy: 0.8947368421052632\n",
    "Precision: 0.9344262295081968\n",
    "Recall: 0.9047619047619048\n",
    "F1 score: 0.9193548387096775\n",
    "Specificity: 0.875\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=['No Match', \"Match\"] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC(fpr, tpr):\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1], [0,1], linestyle='--')\n",
    "    \n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(manualRatedDf[\"Match\"].tolist(), manualRatedDf[\"Score\"].tolist())\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plot_ROC(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = manualRatedDf[['Score', 'tfidfTitle', 'LDAMatch', 'DateDiff']]\n",
    "y = manualRatedDf[\"Match\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test=sklearn.model_selection.train_test_split(X,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = manualRatedDf[['Score', 'tfidfTitle']]\n",
    "y = manualRatedDf[\"Match\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test=sklearn.model_selection.train_test_split(X,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = tree.DecisionTreeClassifier(ccp_alpha=0.005,criterion=\"gini\", max_depth=2, max_features='sqrt', splitter='best')\n",
    "clf = tree.DecisionTreeClassifier(ccp_alpha=0.005,criterion=\"gini\", max_depth=4, splitter='best')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models\\decision_tree_allfeatures', 'rb') as handle:\n",
    "    clf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"F1 score:\",metrics.f1_score(y_test, y_pred))\n",
    "print(\"Specificity:\",(cnf_matrix[0][0]/(cnf_matrix[0][0]+cnf_matrix[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=['No Match', \"Match\"] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"Decision tree, auc=\"+str(auc))\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'criterion':  ['gini', 'entropy'],\n",
    "    'ccp_alpha': [.005, .01, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    'max_depth':  [None, 2,3, 4, 6, 8, 10],\n",
    "    'max_features': [None, 'sqrt', 'log2', 0.2, 0.4, 0.6, 0.8, 1, 2, 3, 4],\n",
    "    'splitter': ['best', 'random']\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    estimator=tree.DecisionTreeClassifier(random_state=0),\n",
    "    scoring='recall',\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    n_jobs=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'ccp_alpha': 0.005, 'criterion': 'gini', 'max_depth': 2, 'max_features': 'sqrt', 'splitter': 'best'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree.plot_tree(clf, filled=True, fontsize=10, feature_names=['Score', 'tfidfTitle', 'LDAMatch', \"DateDiff\"])\n",
    "plt.figure(figsize=(12,12))\n",
    "# plt.savefig('decision_tree_simscore_allfeatures2', dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = clf.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open('models\\decision_tree_allfeatures', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3456e290f23e66e65c86db29512030f5b9518fa8a585a84d9a5b83ccae9d6ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
